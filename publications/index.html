<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/2c4b69791fa865b5.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-62bead22005b90e5.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-2d89fbf9a285c45b.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-5ff14046e29b439a.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-053afab5d2529222.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Chen Zhao</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Chen Zhao"/><meta name="keywords" content="Chen Zhao,PhD,Research,CVLab, EPFL"/><meta name="creator" content="Chen Zhao"/><meta name="publisher" content="Chen Zhao"/><meta property="og:title" content="Chen Zhao"/><meta property="og:description" content="Postdoctoral Researcher at EPFL."/><meta property="og:site_name" content="Chen Zhao&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Chen Zhao"/><meta name="twitter:description" content="Postdoctoral Researcher at EPFL."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Chen Zhao</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/supervising/"><span class="relative z-10">Supervising</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Monte Carlo Diffusion for Generalizable Learning-Based RANSAC" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2026_AAAI_MCD.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Monte Carlo Diffusion for Generalizable Learning-Based RANSAC</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Jiale Wang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">â€ </sup>, </span><span><span class="font-semibold text-accent">Chen Zhao</span><sup class="ml-0 text-accent">â€ </sup>, </span><span><span class="">Wei Ke</span>, </span><span><span class="">Tong Zhang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">AAAI<!-- --> <!-- -->2026</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/comedy0913/MCD" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://comedy0913.github.io/projects/MCD.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2503.09410.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="PhysGen: Physically Grounded 3D Shape Generation for Industrial Design" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2026_CVPR_Shape.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">PhysGen: Physically Grounded 3D Shape Generation for Industrial Design</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yingxuan You</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Hantao Zhang</span>, </span><span><span class="">Mingda Xu</span>, </span><span><span class="">Pascal Fua</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/pdf/2512.00422" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2025_ICCV_boxdreamer.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yuanhong Yu</span>, </span><span><span class="">Xingyi He</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Junhao Yu</span>, </span><span><span class="">Jiaqi Yang</span>, </span><span><span class="">Ruizhen Hu</span>, </span><span><span class="">Yujun Shen</span>, </span><span><span class="">Xing Zhu</span>, </span><span><span class="">Xiaowei Zhou</span>, </span><span><span class="">Sida Peng</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ICCV<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/zju3dv/BoxDreamer" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://zju3dv.github.io/boxdreamer/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2504.07955.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2025_ICCV_SEGS.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Xuan Wang</span>, </span><span><span class="">Tong Zhang</span>, </span><span><span class="">Saqib Javed</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ICCV, Oral<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/SE-GS/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/SEGS.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2411.00144v3.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2025_BMVC_TC3DGS.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Saqib Javed</span>, </span><span><span class="">Ahmad Jarrar Khan</span>, </span><span><span class="">Corentin Dumery</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">BMVC<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/saqibjaved1/TC3DGS" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://ahmad-jarrar.github.io/tc-3dgs/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2412.05700" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2024_CVPR_HOISDF.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Haozhe Qi</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Mathieu Salzmann</span>, </span><span><span class="">Alexander Mathis</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">CVPR<!-- --> <!-- -->2024</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/amathislab/HOISDF" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://arxiv.org/pdf/2402.17062v1.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2024_CVPR_DVMNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Tong Zhang</span>, </span><span><span class="">Zheng Dang</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">CVPR<!-- --> <!-- -->2024</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/DVMNet" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/CVPR2024_DVMNet.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DVMNet_Computing_Relative_Pose_for_Unseen_Objects_Beyond_Hypotheses_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object Pose Estimation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2024_ICLR_3DAHV.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">3D-Aware Hypothesis &amp; Verification for Generalizable Relative Object Pose Estimation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Tong Zhang</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ICLR<!-- --> <!-- -->2024</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/3DAHV" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/ICLR2024_3DAHV.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2310.03534.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Unsupervised 3D Keypoint Discovery with Multi-View Geometry" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2024_3DV_kpt_discovery.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Unsupervised 3D Keypoint Discovery with Multi-View Geometry</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Sina Honari</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Mathieu Salzmann</span>, </span><span><span class="">Pascal Fua</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">3DV<!-- --> <!-- -->2024</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/pdf/2211.12829.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2024_3DV_locposenet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Yinlin Hu</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">3DV<!-- --> <!-- -->2024</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/LocPoseNet" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/3DV2024_LocPoseNet.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/pdf/2211.16290" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Rotation Invariant Point Cloud Analysis: Where Local Geometry Meets Global Topology" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2022_PR_LGRNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Rotation Invariant Point Cloud Analysis: Where Local Geometry Meets Global Topology</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Jiaqi Yang</span>, </span><span><span class="">Xin Xiong</span>, </span><span><span class="">Angfan Zhu</span>, </span><span><span class="">Zhiguo Cao</span>, </span><span><span class="">Xin Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Pattern Recognition<!-- --> <!-- -->2022</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/LGR-Net" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://www.sciencedirect.com/science/article/pii/S0031320322001078" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2022_ECCV_rota.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Yinlin Hu</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ECCV<!-- --> <!-- -->2022</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/Unseen_Object_Pose" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/ECCV2022_Unseen_Object_Pose.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/abs/2203.08472" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2022_ECCV_kpts.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Haocheng Yuan</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Shichao Fan</span>, </span><span><span class="">Jiaxi Jiang</span>, </span><span><span class="">Jiaqi Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ECCV<!-- --> <!-- -->2022</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/pdf/2203.10212.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Progressive Correspondence Pruning by Consensus Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2021_ICCV_CLNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Progressive Correspondence Pruning by Consensus Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Yixiao Ge</span>, </span><span><span class="">Feng Zhu</span>, </span><span><span class="">Rui Zhao</span>, </span><span><span class="">Hongsheng Li</span>, </span><span><span class="">Mathieu Salzmann</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ICCV<!-- --> <!-- -->2021</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/CLNet" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://sailor-z.github.io/projects/ICCV2021_CLNet.html" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Project</a><a href="https://arxiv.org/abs/2101.00591" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Learning to Fuse Local Geometric Features for 3D Rigid Data Matching" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2020_IF_matching.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Learning to Fuse Local Geometric Features for 3D Rigid Data Matching</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Jiaqi Yang</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Ke Xian</span>, </span><span><span class="">Angfan Zhu</span>, </span><span><span class="">Zhiguo Cao</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Information Fusion<!-- --> <!-- -->2020</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://www.sciencedirect.com/science/article/pii/S1566253519301642" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Image Feature Correspondence Selection: A Comparative Study and a New Contribution" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2020_TIP_survey.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Image Feature Correspondence Selection: A Comparative Study and a New Contribution</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Zhiguo Cao</span>, </span><span><span class="">Jiaqi Yang</span>, </span><span><span class="">Ke Xian</span>, </span><span><span class="">Xin Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE Transactions on Image Processing<!-- --> <!-- -->2020</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949766" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Sparse-to-Dense Depth Completion Revisited: Sampling Strategy and Graph Construction" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2020_ECCV_depth.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Sparse-to-Dense Depth Completion Revisited: Sampling Strategy and Graph Construction</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xin Xiong</span>, </span><span><span class="">Haipeng Xiong</span>, </span><span><span class="">Ke Xian</span>, </span><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Zhiguo Cao</span>, </span><span><span class="">Xin Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ECCV<!-- --> <!-- -->2020</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://par.nsf.gov/servlets/purl/10169232" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/2019_CVPR_NMNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Chen Zhao</span>, </span><span><span class="">Zhiguo Cao</span>, </span><span><span class="">Chi Li</span>, </span><span><span class="">Xin Li</span>, </span><span><span class="">Jiaqi Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">CVPR, Oral<!-- --> <!-- -->2019</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://github.com/sailor-z/NM-Net" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_NM-Net_Mining_Reliable_Neighbors_for_Robust_Feature_Correspondences_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-4"><p class="text-xs text-neutral-500 order-2 sm:order-1">Last updated: <!-- -->November 27, 2025</p><div class="order-1 sm:order-2 flex justify-center"><div style="min-height:20px;min-width:100px" class="flex items-center justify-center"></div></div><p class="text-xs text-neutral-500 flex items-center order-3"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-2d89fbf9a285c45b.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-2d89fbf9a285c45b.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-2d89fbf9a285c45b.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/2c4b69791fa865b5.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"x1B0_OD5daJvF3WRf8xfi\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2c4b69791fa865b5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Supervising\",\"type\":\"page\",\"target\":\"supervising\",\"href\":\"/supervising\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"}],\"siteTitle\":\"Chen Zhao\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 27, 2025\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"I_MqpLjYsOaXVCm9VWXeq\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-5ff14046e29b439a.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-053afab5d2529222.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"wang2025monte\",\"title\":\"Monte Carlo Diffusion for Generalizable Learning-Based RANSAC\",\"authors\":[{\"name\":\"Jiale Wang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Wei Ke\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"AAAI\",\"code\":\"https://github.com/comedy0913/MCD\",\"page\":\"https://comedy0913.github.io/projects/MCD.html\",\"pdf\":\"https://arxiv.org/pdf/2503.09410.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2026_AAAI_MCD.png\",\"bibtex\":\"@inproceedings{wang2025monte,\\n  title = {Monte Carlo Diffusion for Generalizable Learning-Based RANSAC},\\n  author = {Wang, Jiale and Zhao, Chen and Ke, Wei and Zhang, Tong},\\n  booktitle = {AAAI},\\n  year = {2026}\\n}\"},{\"id\":\"ying2025physgen\",\"title\":\"PhysGen: Physically Grounded 3D Shape Generation for Industrial Design\",\"authors\":[{\"name\":\"Yingxuan You\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hantao Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mingda Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Pascal Fua\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"arXiv\",\"pdf\":\"https://arxiv.org/pdf/2512.00422\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"2026_CVPR_Shape.png\",\"bibtex\":\"@inproceedings{ying2025physgen,\\n  title = {PhysGen: Physically Grounded 3D Shape Generation for Industrial Design},\\n  author = {You, Yingxuan and Zhao, Chen and Zhang, Hantao and Xu, Mingda and Fua, Pascal},\\n  booktitle = {arXiv},\\n  year = {2025}\\n}\"},{\"id\":\"yu2025boxdreamer\",\"title\":\"BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation\",\"authors\":[{\"name\":\"Yuanhong Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xingyi He\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junhao Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruizhen Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yujun Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xing Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaowei Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sida Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ICCV\",\"code\":\"https://github.com/zju3dv/BoxDreamer\",\"page\":\"https://zju3dv.github.io/boxdreamer/\",\"pdf\":\"https://arxiv.org/pdf/2504.07955.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2025_ICCV_boxdreamer.png\",\"bibtex\":\"@inproceedings{yu2025boxdreamer,\\n  title = {BoxDreamer: Dreaming Box Corners for Generalizable Object Pose Estimation},\\n  author = {Yu, Yuanhong and He, Xingyi and Zhao, Chen and Yu, Junhao and Yang, Jiaqi and Hu, Ruizhen and Shen, Yujun and Zhu, Xing and Zhou, Xiaowei and Peng, Sida},\\n  booktitle = {ICCV},\\n  year = {2025}\\n}\"},{\"id\":\"zhao2025self\",\"title\":\"Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuan Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Saqib Javed\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ICCV, Oral\",\"pages\":\"4940--4950\",\"code\":\"https://github.com/sailor-z/SE-GS/\",\"page\":\"https://sailor-z.github.io/projects/SEGS.html\",\"pdf\":\"https://arxiv.org/pdf/2411.00144v3.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"2025_ICCV_SEGS.png\",\"bibtex\":\"@inproceedings{zhao2025self,\\n  title = {Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis},\\n  author = {Zhao, Chen and Wang, Xuan and Zhang, Tong and Javed, Saqib and Salzmann, Mathieu},\\n  booktitle = {ICCV, Oral},\\n  pages = {4940--4950},\\n  year = {2025}\\n}\"},{\"id\":\"javed2024temporally\",\"title\":\"Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes\",\"authors\":[{\"name\":\"Saqib Javed\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ahmad Jarrar Khan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Corentin Dumery\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"BMVC\",\"code\":\"https://github.com/saqibjaved1/TC3DGS\",\"page\":\"https://ahmad-jarrar.github.io/tc-3dgs/\",\"pdf\":\"https://arxiv.org/pdf/2412.05700\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2025_BMVC_TC3DGS.png\",\"bibtex\":\"@inproceedings{javed2024temporally,\\n  title = {Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes},\\n  author = {Javed, Saqib and Khan, Ahmad Jarrar and Dumery, Corentin and Zhao, Chen and Salzmann, Mathieu},\\n  booktitle = {BMVC},\\n  year = {2025}\\n}\"},{\"id\":\"qi2024hoisdf\",\"title\":\"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields\",\"authors\":[{\"name\":\"Haozhe Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Alexander Mathis\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"CVPR\",\"pages\":\"10392--10402\",\"code\":\"https://github.com/amathislab/HOISDF\",\"pdf\":\"https://arxiv.org/pdf/2402.17062v1.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"2024_CVPR_HOISDF.png\",\"bibtex\":\"@inproceedings{qi2024hoisdf,\\n  title = {HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields},\\n  author = {Qi, Haozhe and Zhao, Chen and Salzmann, Mathieu and Mathis, Alexander},\\n  booktitle = {CVPR},\\n  pages = {10392--10402},\\n  year = {2024},\\n  organization = {IEEE}\\n}\"},{\"id\":\"zhao2024dvmnet\",\"title\":\"DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zheng Dang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"CVPR\",\"pages\":\"20485--20495\",\"code\":\"https://github.com/sailor-z/DVMNet\",\"page\":\"https://sailor-z.github.io/projects/CVPR2024_DVMNet.html\",\"pdf\":\"https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DVMNet_Computing_Relative_Pose_for_Unseen_Objects_Beyond_Hypotheses_CVPR_2024_paper.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"2024_CVPR_DVMNet.png\",\"bibtex\":\"@inproceedings{zhao2024dvmnet,\\n  title = {DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses},\\n  author = {Zhao, Chen and Zhang, Tong and Dang, Zheng and Salzmann, Mathieu},\\n  booktitle = {CVPR},\\n  pages = {20485--20495},\\n  year = {2024},\\n  organization = {IEEE}\\n}\"},{\"id\":\"zhao20233d\",\"title\":\"3D-Aware Hypothesis \u0026 Verification for Generalizable Relative Object Pose Estimation\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:7:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ICLR\",\"conference\":\"\",\"code\":\"https://github.com/sailor-z/3DAHV\",\"page\":\"https://sailor-z.github.io/projects/ICLR2024_3DAHV.html\",\"pdf\":\"https://arxiv.org/pdf/2310.03534.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2024_ICLR_3DAHV.png\",\"bibtex\":\"@inproceedings{zhao20233d,\\n  title = {3D-Aware Hypothesis \\\\\u0026 Verification for Generalizable Relative Object Pose Estimation},\\n  author = {Zhao, Chen and Zhang, Tong and Salzmann, Mathieu},\\n  journal = {ICLR},\\n  year = {2024}\\n}\"},{\"id\":\"honari2024unsupervised\",\"title\":\"Unsupervised 3D Keypoint Discovery with Multi-View Geometry\",\"authors\":[{\"name\":\"Sina Honari\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Pascal Fua\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:8:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"3DV\",\"pages\":\"1584--1593\",\"pdf\":\"https://arxiv.org/pdf/2211.12829.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2024_3DV_kpt_discovery.png\",\"bibtex\":\"@inproceedings{honari2024unsupervised,\\n  title = {Unsupervised 3D Keypoint Discovery with Multi-View Geometry},\\n  author = {Honari, Sina and Zhao, Chen and Salzmann, Mathieu and Fua, Pascal},\\n  booktitle = {3DV},\\n  pages = {1584--1593},\\n  year = {2024},\\n  organization = {IEEE}\\n}\"},{\"id\":\"zhao2022locposenet\",\"title\":\"LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinlin Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:9:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"3DV\",\"code\":\"https://github.com/sailor-z/LocPoseNet\",\"page\":\"https://sailor-z.github.io/projects/3DV2024_LocPoseNet.html\",\"pdf\":\"https://arxiv.org/pdf/2211.16290\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2024_3DV_locposenet.png\",\"bibtex\":\"@inproceedings{zhao2022locposenet,\\n  title = {LocPoseNet: Robust Location Prior for Unseen Object Pose Estimation},\\n  author = {Zhao, Chen and Hu, Yinlin and Salzmann, Mathieu},\\n  booktitle = {3DV},\\n  year = {2024},\\n  organization = {IEEE}\\n}\"},{\"id\":\"zhao2022rotation\",\"title\":\"Rotation Invariant Point Cloud Analysis: Where Local Geometry Meets Global Topology\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Angfan Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiguo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:10:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Pattern Recognition\",\"conference\":\"\",\"volume\":\"127\",\"pages\":\"108626\",\"code\":\"https://github.com/sailor-z/LGR-Net\",\"pdf\":\"https://www.sciencedirect.com/science/article/pii/S0031320322001078\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2022_PR_LGRNet.png\",\"bibtex\":\"@article{zhao2022rotation,\\n  title = {Rotation Invariant Point Cloud Analysis: Where Local Geometry Meets Global Topology},\\n  author = {Zhao, Chen and Yang, Jiaqi and Xiong, Xin and Zhu, Angfan and Cao, Zhiguo and Li, Xin},\\n  journal = {Pattern Recognition},\\n  volume = {127},\\n  pages = {108626},\\n  year = {2022},\\n  publisher = {Elsevier}\\n}\"},{\"id\":\"zhao2022fusing\",\"title\":\"Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinlin Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:11:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ECCV\",\"pages\":\"106--122\",\"code\":\"https://github.com/sailor-z/Unseen_Object_Pose\",\"page\":\"https://sailor-z.github.io/projects/ECCV2022_Unseen_Object_Pose.html\",\"pdf\":\"https://arxiv.org/abs/2203.08472\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2022_ECCV_rota.png\",\"bibtex\":\"@inproceedings{zhao2022fusing,\\n  title = {Fusing Local Similarities for Retrieval-Based 3D Orientation Estimation of Unseen Objects},\\n  author = {Zhao, Chen and Hu, Yinlin and Salzmann, Mathieu},\\n  booktitle = {ECCV},\\n  pages = {106--122},\\n  year = {2022},\\n  organization = {Springer}\\n}\"},{\"id\":\"yuan2022unsupervised\",\"title\":\"Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction\",\"authors\":[{\"name\":\"Haocheng Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shichao Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaxi Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:12:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ECCV\",\"pages\":\"534--549\",\"pdf\":\"https://arxiv.org/pdf/2203.10212.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2022_ECCV_kpts.png\",\"bibtex\":\"@inproceedings{yuan2022unsupervised,\\n  title = {Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction},\\n  author = {Yuan, Haocheng and Zhao, Chen and Fan, Shichao and Jiang, Jiaxi and Yang, Jiaqi},\\n  booktitle = {ECCV},\\n  pages = {534--549},\\n  year = {2022},\\n  organization = {Springer}\\n}\"},{\"id\":\"zhao2021progressive\",\"title\":\"Progressive Correspondence Pruning by Consensus Learning\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yixiao Ge\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feng Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rui Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongsheng Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mathieu Salzmann\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:13:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ICCV\",\"pages\":\"6464--6473\",\"code\":\"https://github.com/sailor-z/CLNet\",\"page\":\"https://sailor-z.github.io/projects/ICCV2021_CLNet.html\",\"pdf\":\"https://arxiv.org/abs/2101.00591\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2021_ICCV_CLNet.png\",\"bibtex\":\"@inproceedings{zhao2021progressive,\\n  title = {Progressive Correspondence Pruning by Consensus Learning},\\n  author = {Zhao, Chen and Ge, Yixiao and Zhu, Feng and Zhao, Rui and Li, Hongsheng and Salzmann, Mathieu},\\n  booktitle = {ICCV},\\n  pages = {6464--6473},\\n  year = {2021},\\n  organization = {IEEE}\\n}\"},{\"id\":\"yang2020learning\",\"title\":\"Learning to Fuse Local Geometric Features for 3D Rigid Data Matching\",\"authors\":[{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Xian\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Angfan Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiguo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2020,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:14:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Information Fusion\",\"conference\":\"\",\"volume\":\"61\",\"pages\":\"24--35\",\"pdf\":\"https://www.sciencedirect.com/science/article/pii/S1566253519301642\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2020_IF_matching.png\",\"bibtex\":\"@article{yang2020learning,\\n  title = {Learning to Fuse Local Geometric Features for 3D Rigid Data Matching},\\n  author = {Yang, Jiaqi and Zhao, Chen and Xian, Ke and Zhu, Angfan and Cao, Zhiguo},\\n  journal = {Information Fusion},\\n  volume = {61},\\n  pages = {24--35},\\n  year = {2020},\\n  publisher = {Elsevier}\\n}\"},{\"id\":\"zhao2020image\",\"title\":\"Image Feature Correspondence Selection: A Comparative Study and a New Contribution\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiguo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Xian\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2020,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:15:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE Transactions on Image Processing\",\"conference\":\"\",\"volume\":\"29\",\"pages\":\"3506--3519\",\"pdf\":\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949766\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2020_TIP_survey.png\",\"bibtex\":\"@article{zhao2020image,\\n  title = {Image Feature Correspondence Selection: A Comparative Study and a New Contribution},\\n  author = {Zhao, Chen and Cao, Zhiguo and Yang, Jiaqi and Xian, Ke and Li, Xin},\\n  journal = {IEEE Transactions on Image Processing},\\n  volume = {29},\\n  pages = {3506--3519},\\n  year = {2020},\\n  publisher = {IEEE}\\n}\"},{\"id\":\"xiong2020sparse\",\"title\":\"Sparse-to-Dense Depth Completion Revisited: Sampling Strategy and Graph Construction\",\"authors\":[{\"name\":\"Xin Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haipeng Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ke Xian\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiguo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2020,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:16:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ECCV\",\"pages\":\"682--699\",\"pdf\":\"https://par.nsf.gov/servlets/purl/10169232\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"2020_ECCV_depth.png\",\"bibtex\":\"@inproceedings{xiong2020sparse,\\n  title = {Sparse-to-Dense Depth Completion Revisited: Sampling Strategy and Graph Construction},\\n  author = {Xiong, Xin and Xiong, Haipeng and Xian, Ke and Zhao, Chen and Cao, Zhiguo and Li, Xin},\\n  booktitle = {ECCV},\\n  pages = {682--699},\\n  year = {2020},\\n  organization = {Springer}\\n}\"},{\"id\":\"zhao2019nm\",\"title\":\"NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences\",\"authors\":[{\"name\":\"Chen Zhao\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiguo Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2019,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:17:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"CVPR, Oral\",\"pages\":\"215--224\",\"code\":\"https://github.com/sailor-z/NM-Net\",\"pdf\":\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_NM-Net_Mining_Reliable_Neighbors_for_Robust_Feature_Correspondences_CVPR_2019_paper.pdf\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"2019_CVPR_NMNet.png\",\"bibtex\":\"@inproceedings{zhao2019nm,\\n  title = {NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences},\\n  author = {Zhao, Chen and Cao, Zhiguo and Li, Chi and Li, Xin and Yang, Jiaqi},\\n  booktitle = {CVPR, Oral},\\n  pages = {215--224},\\n  year = {2019},\\n  organization = {IEEE}\\n}\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Chen Zhao\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Chen Zhao\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Chen Zhao,PhD,Research,CVLab, EPFL\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Chen Zhao\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Chen Zhao\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Chen Zhao\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Postdoctoral Researcher at EPFL.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Chen Zhao's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Chen Zhao\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Postdoctoral Researcher at EPFL.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>